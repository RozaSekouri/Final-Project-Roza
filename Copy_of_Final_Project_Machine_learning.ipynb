{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RozaSekouri/Final-Project-Roza/blob/main/Copy_of_Final_Project_Machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoEoCa_8cXC4"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!pip install kagglehub\n",
        "!pip install wordcloud\n",
        "!pip install nltk\n",
        "!pip install torch -qq\n",
        "!pip install pyarrow -qq\n",
        "!pip install transformers accelerate evaluate datasets -qq\n",
        "import pandas as pd\n",
        "import os\n",
        "import kagglehub\n",
        "import ast\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import evaluate\n",
        "from datasets import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loRV_-uYfRco"
      },
      "source": [
        "Generate Kaggle API Token:\n",
        "\n",
        "Go to Kaggle.\n",
        "\n",
        "Log in to your account.\n",
        "\n",
        "Click on your profile picture in the top right corner and select \"My Account\".\n",
        "\n",
        "Scroll down to the \"API\" section and click \"Create New API Token\". This will download a kaggle.json file to your computer.\n",
        "\n",
        "Upload kaggle.json to Colab:\n",
        "\n",
        "In your Colab notebook, run the following cell. It will prompt you to upload the kaggle.json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83NBk-Ope-hh"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VCD1m-RfE47"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DimskxUFd7ar"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d jiashenliu/515k-hotel-reviews-data-in-europe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukAiArABfYhi"
      },
      "outputs": [],
      "source": [
        "!unzip 515k-hotel-reviews-data-in-europe.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRGdtwHbfwoh"
      },
      "outputs": [],
      "source": [
        "# Define the path to unzipped CSV file\n",
        "file_path = 'Hotel_Reviews.csv'\n",
        "\n",
        "# Load the CSV into a Pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "    print(\"DataFrame loaded successfully!\")\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDataFrame Info (columns, non-null counts, dtypes):\")\n",
        "    df.info()\n",
        "\n",
        "    print(f\"\\nTotal rows in DataFrame: {df.shape[0]}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found.\")\n",
        "    print(\"Please ensure you unzipped the file correctly and it's in the current directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the DataFrame: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-JFAsdOf8SY"
      },
      "outputs": [],
      "source": [
        "# Show the number of rows and columns (DataFrame shape)\n",
        "print(\"DataFrame Shape (Rows, Columns):\")\n",
        "print(df.shape)\n",
        "\n",
        "# Show the names of all columns\n",
        "print(\"\\nDataFrame Column Names:\")\n",
        "print(df.columns.tolist()) # .tolist() makes it print as a neat list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwAVGKMXgIBZ"
      },
      "outputs": [],
      "source": [
        "print(\"\\nDetailed Missing Values Count:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duUn0zz1gJKx"
      },
      "outputs": [],
      "source": [
        "print(\"\\nNumber of Duplicate Rows:\")\n",
        "num_duplicates = df.duplicated().sum()\n",
        "print(f\"Found {num_duplicates} duplicate rows.\")\n",
        "\n",
        "if num_duplicates > 0:\n",
        "    print(\"Removing duplicate rows...\")\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    print(f\"Duplicates removed. New DataFrame shape: {df.shape}\")\n",
        "else:\n",
        "    print(\"No duplicate rows found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCe9rp0LgOWZ"
      },
      "outputs": [],
      "source": [
        "df['Positive_Review_Clean'] = df['Positive_Review'].astype(str).replace('No Positive', '')\n",
        "df['Negative_Review_Clean'] = df['Negative_Review'].astype(str).replace('No Negative', '')\n",
        "\n",
        "# Concatenate positive and negative reviews to form a single 'Review_Text' column.\n",
        "# Add a space in between if both are present.\n",
        "df['Review_Text'] = df['Positive_Review_Clean'] + ' ' + df['Negative_Review_Clean']\n",
        "\n",
        "# Remove leading/trailing spaces and handle multiple spaces\n",
        "df['Review_Text'] = df['Review_Text'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "# Drop the temporary clean columns if no longer needed\n",
        "df.drop(columns=['Positive_Review_Clean', 'Negative_Review_Clean'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOzfbUmBgRRg"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NT97jFbgVX5"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atIoMMIfgWd5"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO89t_mEgZug"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpERtjErgcix"
      },
      "outputs": [],
      "source": [
        "# Displaying top 20 unique hotel names, as there will be many\n",
        "print(df['Hotel_Name'].value_counts().head(20))\n",
        "print(f\"\\nTotal unique hotels: {df['Hotel_Name'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-6TRweCgfWY"
      },
      "outputs": [],
      "source": [
        "# Displaying top 20 unique nationalities for brevity, as there might be many\n",
        "print(df['Reviewer_Nationality'].value_counts().head(20))\n",
        "print(f\"\\nTotal unique nationalities: {df['Reviewer_Nationality'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qdcMJB8gjrn"
      },
      "outputs": [],
      "source": [
        "# The 'Tags' column is a string containing multiple tags separated by commas.\n",
        "# To get truly unique individual tags, we need to process this column.\n",
        "# Let's show a few raw examples first.\n",
        "print(\"\\nExample raw 'Tags' entries:\")\n",
        "print(df['Tags'].head().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBIhWWPegmDw"
      },
      "outputs": [],
      "source": [
        "# To get actual individual unique tags, we need to split and explode the strings\n",
        "# This process might be computationally intensive for very large datasets if not optimized.\n",
        "# Let's get a sample of common tags first\n",
        "# This is a good step for later preprocessing, just showing a few unique combinations for now.\n",
        "print(f\"\\nTotal unique combinations of tags: {df['Tags'].nunique()}\")\n",
        "print(\"Top 100 most frequent tag combinations:\")\n",
        "df['Tags'].value_counts().head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjhaPQ3bgnbQ"
      },
      "outputs": [],
      "source": [
        "# --- Analyze the Reviewer_Score distribution ---\n",
        "print(\"\\n--- Distribution of 'Reviewer_Score' ---\")\n",
        "print(df['Reviewer_Score'].value_counts().sort_index())\n",
        "print(f\"\\nMean Reviewer Score: {df['Reviewer_Score'].mean():.2f}\")\n",
        "print(f\"Median Reviewer Score: {df['Reviewer_Score'].median():.2f}\")\n",
        "print(f\"Min Reviewer Score: {df['Reviewer_Score'].min():.2f}\")\n",
        "print(f\"Max Reviewer Score: {df['Reviewer_Score'].max():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaXh3TTGgvxC"
      },
      "outputs": [],
      "source": [
        "# We can also visualize this distribution with a histogram\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Reviewer_Score'], bins=20, kde=True)\n",
        "plt.title('Distribution of Reviewer Scores')\n",
        "plt.xlabel('Reviewer Score')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhHEPZuwg30Z"
      },
      "source": [
        "# --- Step 1: Convert 'Tags' string to an actual list of strings ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u14yrz-g3Ip"
      },
      "outputs": [],
      "source": [
        "# Use ast.literal_eval to safely parse the string representation of a list\n",
        "def parse_tags_string(tags_str):\n",
        "    try:\n",
        "        # Evaluate the string as a Python literal (list in this case)\n",
        "        # Then strip whitespace from each tag\n",
        "        return [tag.strip() for tag in ast.literal_eval(tags_str)]\n",
        "    except (ValueError, SyntaxError):\n",
        "        return [] # Return empty list if parsing fails for any reason\n",
        "\n",
        "df['Tags_List'] = df['Tags'].apply(parse_tags_string)\n",
        "\n",
        "print(\"\\n--- Example of 'Tags_List' after parsing ---\")\n",
        "print(df[['Tags', 'Tags_List']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrDoqla6g--J"
      },
      "source": [
        "# --- Step 2: Define functions to extract specific information ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-Bu0W6NhAqY"
      },
      "outputs": [],
      "source": [
        "def extract_group_size(tags_list):\n",
        "    group_size_keywords = {\n",
        "        'Couple': 'Couple',\n",
        "        'Solo traveler': 'Solo',\n",
        "        'Family with young children': 'Family',\n",
        "        'Family with older children': 'Family',\n",
        "        'Group': 'Group',\n",
        "        'Friends': 'Friends',\n",
        "        'Business traveler': 'Business'\n",
        "    }\n",
        "    for tag in tags_list:\n",
        "        for keyword, category in group_size_keywords.items():\n",
        "            if keyword in tag:\n",
        "                return category\n",
        "    return 'Unknown Group' # Default if no match is found\n",
        "\n",
        "def extract_room_type(tags_list):\n",
        "    # Prioritize more specific room types first\n",
        "    room_type_keywords = [\n",
        "        'Duplex Double Room', 'Family Room', 'King Room', 'Queen Room',\n",
        "        'Superior Double Room', 'Deluxe Double Room', 'Standard Double Room',\n",
        "        'Twin Room', 'Double Room', 'Single Room', 'Triple Room', 'Quadruple Room',\n",
        "        'Suite', 'Apartment', 'Studio', 'Executive Room', 'Club Room',\n",
        "        'Standard Room', 'Superior Room', 'Deluxe Room', 'Classic Room'\n",
        "    ]\n",
        "    for tag in tags_list:\n",
        "        for keyword in room_type_keywords:\n",
        "            if keyword in tag:\n",
        "                return keyword.replace(' Room', '').strip() # Clean up ' Room' for consistency\n",
        "    return 'Unknown Room Type' # Default if no match\n",
        "\n",
        "def extract_length_of_stay(tags_list):\n",
        "    for tag in tags_list:\n",
        "        match = re.search(r'Stayed (\\d+) night(?:s)?', tag)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "    return None # Return None if length of stay isn't found\n",
        "\n",
        "def extract_trip_type(tags_list):\n",
        "    trip_type_keywords = {\n",
        "        'Leisure trip': 'Leisure',\n",
        "        'Business trip': 'Business'\n",
        "    }\n",
        "    for tag in tags_list:\n",
        "        for keyword, category in trip_type_keywords.items():\n",
        "            if keyword in tag:\n",
        "                return category\n",
        "    return 'Unknown Trip Type'\n",
        "\n",
        "def extract_submission_device(tags_list):\n",
        "    for tag in tags_list:\n",
        "        if 'Submitted from a mobile device' in tag:\n",
        "            return 'Mobile'\n",
        "        elif 'Submitted from a desktop device' in tag: # Assuming this tag might exist, though less common\n",
        "            return 'Desktop'\n",
        "    return 'Unknown Device'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XFNcjzohEuA"
      },
      "source": [
        "# --- Step 3: Apply the extraction functions to create new columns ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxGUSSlJhHlB"
      },
      "outputs": [],
      "source": [
        "df['group_size'] = df['Tags_List'].apply(extract_group_size)\n",
        "df['room_type'] = df['Tags_List'].apply(extract_room_type)\n",
        "df['length_of_stay'] = df['Tags_List'].apply(extract_length_of_stay)\n",
        "df['trip_type'] = df['Tags_List'].apply(extract_trip_type)\n",
        "df['submission_device'] = df['Tags_List'].apply(extract_submission_device)\n",
        "\n",
        "\n",
        "print(\"\\n--- DataFrame with new extracted columns (head) ---\")\n",
        "print(df[['Tags', 'group_size', 'room_type', 'length_of_stay', 'trip_type', 'submission_device']].head())\n",
        "\n",
        "print(\"\\n--- Value Counts for New Columns ---\")\n",
        "print(\"\\nGroup Size Distribution:\")\n",
        "print(df['group_size'].value_counts())\n",
        "\n",
        "print(\"\\nRoom Type Distribution (Top 20):\")\n",
        "print(df['room_type'].value_counts().head(20))\n",
        "\n",
        "print(\"\\nLength of Stay Distribution (Top 10):\")\n",
        "print(df['length_of_stay'].value_counts().head(10))\n",
        "\n",
        "print(\"\\nTrip Type Distribution:\")\n",
        "print(df['trip_type'].value_counts())\n",
        "\n",
        "print(\"\\nSubmission Device Distribution:\")\n",
        "print(df['submission_device'].value_counts())\n",
        "\n",
        "print(\"\\nDataFrame Info after adding new columns:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaU78_eDhQM4"
      },
      "outputs": [],
      "source": [
        "# --- Numerical Features Histograms ---\n",
        "numerical_cols = [\n",
        "    'Review_Total_Negative_Word_Counts',\n",
        "    'Review_Total_Positive_Word_Counts',\n",
        "    'Total_Number_of_Reviews',\n",
        "    'Total_Number_of_Reviews_Reviewer_Has_Given',\n",
        "    'Average_Score',\n",
        "    'days_since_review',\n",
        "    'length_of_stay'\n",
        "]\n",
        "\n",
        "print(\"\\n--- Histograms for Numerical Features ---\")\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20))\n",
        "axes = axes.flatten() # Flatten for easy iteration\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    sns.histplot(df[col].dropna(), kde=True, ax=axes[i], bins=30)\n",
        "    axes[i].set_title(f'Distribution of {col}')\n",
        "    axes[i].set_xlabel(col)\n",
        "    axes[i].set_ylabel('Count')\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Categorical Features Bar Plots ---\n",
        "categorical_to_plot = [\n",
        "    'group_size', 'room_type', 'trip_type', 'submission_device'\n",
        "]\n",
        "\n",
        "print(\"\\n--- Bar Plots for Extracted Categorical Features ---\")\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(categorical_to_plot):\n",
        "    # For room_type, take top N to avoid clutter\n",
        "    if col == 'room_type':\n",
        "        sns.countplot(y=df[col], order=df[col].value_counts().index[:10], ax=axes[i], palette='viridis', hue=df[col], legend=False)\n",
        "        axes[i].set_title(f'Top 10 Distribution of {col}')\n",
        "    else:\n",
        "        sns.countplot(y=df[col], order=df[col].value_counts().index, ax=axes[i], palette='viridis', hue=df[col], legend=False)\n",
        "        axes[i].set_title(f'Distribution of {col}')\n",
        "    axes[i].set_xlabel('Count')\n",
        "    axes[i].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Top Reviewer Nationalities ---\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(y='Reviewer_Nationality', data=df, order=df['Reviewer_Nationality'].value_counts().head(20).index, palette='crest', hue='Reviewer_Nationality', legend=False)\n",
        "plt.title('Top 20 Reviewer Nationalities')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Nationality')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_Zd6vUdhVAo"
      },
      "outputs": [],
      "source": [
        "# Scatter plots for continuous variables\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "sns.scatterplot(x='Review_Total_Negative_Word_Counts', y='Reviewer_Score', data=df, ax=axes[0], alpha=0.1)\n",
        "axes[0].set_title('Reviewer Score vs. Negative Word Counts')\n",
        "axes[0].set_xlim(0, 100)\n",
        "\n",
        "sns.scatterplot(x='Review_Total_Positive_Word_Counts', y='Reviewer_Score', data=df, ax=axes[1], alpha=0.1)\n",
        "axes[1].set_title('Reviewer Score vs. Positive Word Counts')\n",
        "axes[1].set_xlim(0, 100)\n",
        "\n",
        "sns.scatterplot(x='Total_Number_of_Reviews_Reviewer_Has_Given', y='Reviewer_Score', data=df, ax=axes[2], alpha=0.05)\n",
        "axes[2].set_title('Reviewer Score vs. Reviewer Total Reviews')\n",
        "axes[2].set_xlim(0, 50)\n",
        "\n",
        "sns.boxplot(x='Reviewer_Score', y='Average_Score', data=df, ax=axes[3]) # Box plot, as Average_Score is hotel-level\n",
        "axes[3].set_title('Average Hotel Score by Individual Reviewer Score')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxdGhhathYKY"
      },
      "outputs": [],
      "source": [
        "# --- Reviewer_Score vs. Categorical Features (Box Plots) ---\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(18, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Assign hue to the same column as x, and set legend=False\n",
        "sns.boxplot(x='group_size', y='Reviewer_Score', data=df, ax=axes[0], palette='pastel', hue='group_size', legend=False)\n",
        "axes[0].set_title('Reviewer Score by Group Size')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Assign hue to the same column as x, and set legend=False\n",
        "sns.boxplot(x='trip_type', y='Reviewer_Score', data=df, ax=axes[1], palette='pastel', hue='trip_type', legend=False)\n",
        "axes[1].set_title('Reviewer Score by Trip Type')\n",
        "\n",
        "top_room_types = df['room_type'].value_counts().head(10).index\n",
        "# Assign hue to the same column as x, and set legend=False\n",
        "sns.boxplot(x='room_type', y='Reviewer_Score', data=df[df['room_type'].isin(top_room_types)], ax=axes[2], palette='pastel', hue='room_type', legend=False)\n",
        "axes[2].set_title('Reviewer Score by Top 10 Room Types')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "top_nationalities = df['Reviewer_Nationality'].value_counts().head(10).index\n",
        "# Assign hue to the same column as x, and set legend=False\n",
        "sns.boxplot(x='Reviewer_Nationality', y='Reviewer_Score', data=df[df['Reviewer_Nationality'].isin(top_nationalities)], ax=axes[3], palette='pastel', hue='Reviewer_Nationality', legend=False)\n",
        "axes[3].set_title('Reviewer Score by Top 10 Nationalities')\n",
        "axes[3].tick_params(axis='x', rotation=90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gLbdxNmherv"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix for numerical features\n",
        "print(\"\\n--- Correlation Matrix of Numerical Features ---\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = df[numerical_cols + ['Reviewer_Score']].corr(numeric_only=True)\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix of Numerical Features and Reviewer Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_Bvj055hiQn"
      },
      "outputs": [],
      "source": [
        "# --- Geographical Analysis ---\n",
        "\n",
        "print(\"\\n--- Geographical Analysis ---\")\n",
        "\n",
        "def extract_city_country(address):\n",
        "    # This heuristic assumes the predefined cities are in the address\n",
        "    if 'London' in address: return 'London', 'United Kingdom'\n",
        "    if 'Paris' in address: return 'Paris', 'France'\n",
        "    if 'Milan' in address: return 'Milan', 'Italy'\n",
        "    if 'Amsterdam' in address: return 'Amsterdam', 'Netherlands'\n",
        "    if 'Barcelona' in address: return 'Barcelona', 'Spain'\n",
        "    if 'Vienna' in address: return 'Vienna', 'Austria'\n",
        "    return 'Unknown City', 'Unknown Country'\n",
        "\n",
        "df['City'], df['Country'] = zip(*df['Hotel_Address'].apply(extract_city_country))\n",
        "\n",
        "print(\"\\nCity and Country columns extracted.\")\n",
        "print(\"\\nCity Distribution:\")\n",
        "print(df['City'].value_counts())\n",
        "\n",
        "print(\"\\nCountry Distribution:\")\n",
        "print(df['Country'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# ADDED hue='City' and legend=False\n",
        "sns.boxplot(x='City', y='Reviewer_Score', data=df, palette='Spectral', hue='City', legend=False)\n",
        "plt.title('Reviewer Score by City')\n",
        "plt.xlabel('City')\n",
        "plt.ylabel('Reviewer Score')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# ADDED hue='Country' and legend=False\n",
        "sns.boxplot(x='Country', y='Reviewer_Score', data=df, palette='Spectral', hue='Country', legend=False)\n",
        "plt.title('Reviewer Score by Country')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Reviewer Score')\n",
        "plt.show()\n",
        "\n",
        "avg_score_per_hotel = df.groupby('Hotel_Name')['Reviewer_Score'].mean().reset_index()\n",
        "avg_score_per_hotel = avg_score_per_hotel.sort_values(by='Reviewer_Score', ascending=False)\n",
        "print(\"\\nTop 10 Hotels by Average Reviewer Score:\")\n",
        "print(avg_score_per_hotel.head(10))\n",
        "print(\"\\nBottom 10 Hotels by Average Reviewer Score:\")\n",
        "print(avg_score_per_hotel.tail(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REq8-hkShoVg"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"NLTK 'stopwords' corpus already downloaded.\")\n",
        "except LookupError: # Catch LookupError as raised by nltk.data.find()\n",
        "    nltk.download('stopwords')\n",
        "    print(\"NLTK 'stopwords' corpus downloaded.\")\n",
        "\n",
        "# Replace 'No Negative' and 'No Positive' with empty strings\n",
        "df['Negative_Review'] = df['Negative_Review'].replace('No Negative', '')\n",
        "df['Positive_Review'] = df['Positive_Review'].replace('No Positive', '')\n",
        "df['Review_Text'] = df['Positive_Review'] + ' ' + df['Negative_Review']\n",
        "\n",
        "print(\"\\n'Review_Text' column created by combining Positive and Negative reviews.\")\n",
        "print(df[['Positive_Review', 'Negative_Review', 'Review_Text']].head())\n",
        "\n",
        "# Create a 'sentiment' target variable\n",
        "df['Sentiment'] = df['Reviewer_Score'].apply(lambda score: 'Positive' if score >= 7 else 'Negative')\n",
        "print(\"\\n'Sentiment' column created based on Reviewer_Score.\")\n",
        "print(df['Sentiment'].value_counts())\n",
        "\n",
        "# Word Cloud for Positive Reviews\n",
        "print(\"\\n--- Word Cloud for Positive Reviews ---\")\n",
        "positive_reviews_text = \" \".join(review for review in df[df['Sentiment'] == 'Positive']['Review_Text'])\n",
        "wordcloud_positive = WordCloud(width=800, height=400, background_color='white',\n",
        "                               stopwords=set(nltk.corpus.stopwords.words('english'))).generate(positive_reviews_text)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud for Positive Reviews')\n",
        "plt.show()\n",
        "\n",
        "# Word Cloud for Negative Reviews\n",
        "print(\"\\n--- Word Cloud for Negative Reviews ---\")\n",
        "negative_reviews_text = \" \".join(review for review in df[df['Sentiment'] == 'Negative']['Review_Text'])\n",
        "wordcloud_negative = WordCloud(width=800, height=400, background_color='black',\n",
        "                               stopwords=set(nltk.corpus.stopwords.words('english'))).generate(negative_reviews_text)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud for Negative Reviews')\n",
        "plt.show()\n",
        "\n",
        "# Review Length Analysis\n",
        "df['review_length'] = df['Review_Text'].apply(len)\n",
        "print(\"\\nAverage Review Length by Sentiment:\")\n",
        "print(df.groupby('Sentiment')['review_length'].mean())\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# ADDED hue='Sentiment' and legend=False for consistency\n",
        "sns.boxplot(x='Sentiment', y='review_length', data=df, hue='Sentiment', legend=False)\n",
        "plt.title('Review Length by Sentiment')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Review Length (characters)')\n",
        "plt.ylim(0, 500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQaYURNuOd4o"
      },
      "source": [
        "#--- Deep dive further in the word analysis ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk0UL_tYOkan"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"NLTK 'stopwords' corpus already downloaded.\")\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    print(\"NLTK 'stopwords' corpus downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JYMalGDO1LX"
      },
      "outputs": [],
      "source": [
        "# Remove the drop_duplicates line as duplicates were handled earlier\n",
        "# df.drop_duplicates(inplace=True)\n",
        "\n",
        "df['Positive_Review_Clean'] = df['Positive_Review'].astype(str).replace('No Positive', '')\n",
        "df['Negative_Review_Clean'] = df['Negative_Review'].astype(str).replace('No Negative', '')\n",
        "df['Review_Text'] = df['Positive_Review_Clean'] + ' ' + df['Negative_Review_Clean']\n",
        "df['Review_Text'] = df['Review_Text'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "df.drop(columns=['Positive_Review_Clean', 'Negative_Review_Clean'], inplace=True)\n",
        "\n",
        "# Create 'Sentiment' target variable (from your previous steps)\n",
        "df['Sentiment'] = df['Reviewer_Score'].apply(lambda score: 'Positive' if score >= 7 else 'Negative')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkdoIhDFPTW3"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing Function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "print(\"\\nApplying text preprocessing to 'Review_Text'...\")\n",
        "df['Review_Text_Clean'] = df['Review_Text'].apply(preprocess_text)\n",
        "print(\"Text preprocessing complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRhiz2RCPkQ4"
      },
      "outputs": [],
      "source": [
        "# --- TF-IDF Vectorization with Unigrams, Bigrams, and Trigrams ---\n",
        "print(\"\\nStarting TF-IDF Vectorization with Unigrams, Bigrams, and Trigrams...\")\n",
        "# Changed ngram_range to (1, 3) to include trigrams\n",
        "tfidf_vectorizer_trigrams = TfidfVectorizer(max_features=15000, ngram_range=(1, 3)) # Increased max_features slightly due to more n-grams\n",
        "\n",
        "# Fit and transform the cleaned review text\n",
        "X_tfidf_trigrams = tfidf_vectorizer_trigrams.fit_transform(df['Review_Text_Clean'])\n",
        "\n",
        "print(f\"\\nShape of TF-IDF matrix (with trigrams): {X_tfidf_trigrams.shape}\")\n",
        "print(f\"Number of features (unique words/n-grams): {len(tfidf_vectorizer_trigrams.get_feature_names_out())}\")\n",
        "print(\"\\nTF-IDF vectorization with trigrams complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVrt7AdFT2iI"
      },
      "outputs": [],
      "source": [
        "# Filter reviews by sentiment\n",
        "positive_reviews_list = df[df['Sentiment'] == 'Positive']['Review_Text_Clean'].tolist()\n",
        "negative_reviews_list = df[df['Sentiment'] == 'Negative']['Review_Text_Clean'].tolist()\n",
        "\n",
        "# Handle cases where one of the lists might be empty after filtering (e.g., if a sentiment has no reviews)\n",
        "if not positive_reviews_list:\n",
        "    print(\"Warning: No positive reviews found after cleaning and filtering. Cannot generate positive trigram word cloud.\")\n",
        "    positive_trigram_dict = {} # Set to empty dict to avoid errors later\n",
        "else:\n",
        "    # Initialize CountVectorizer for trigrams only\n",
        "    ngram_vectorizer_pos = CountVectorizer(ngram_range=(3, 3), max_features=2000)\n",
        "    X_positive_trigrams = ngram_vectorizer_pos.fit_transform(positive_reviews_list)\n",
        "    positive_trigram_counts = X_positive_trigrams.sum(axis=0)\n",
        "    positive_trigram_freq = [(word, positive_trigram_counts[0, idx]) for word, idx in ngram_vectorizer_pos.vocabulary_.items()]\n",
        "    positive_trigram_freq = sorted(positive_trigram_freq, key=lambda x: x[1], reverse=True)\n",
        "    positive_trigram_dict = dict(positive_trigram_freq)\n",
        "\n",
        "\n",
        "if not negative_reviews_list:\n",
        "    print(\"Warning: No negative reviews found after cleaning and filtering. Cannot generate negative trigram word cloud.\")\n",
        "    negative_trigram_dict = {} # Set to empty dict\n",
        "else:\n",
        "    # Initialize CountVectorizer for trigrams only (use a new instance for negative reviews)\n",
        "    ngram_vectorizer_neg = CountVectorizer(ngram_range=(3, 3), max_features=2000)\n",
        "    X_negative_trigrams = ngram_vectorizer_neg.fit_transform(negative_reviews_list)\n",
        "    negative_trigram_counts = X_negative_trigrams.sum(axis=0)\n",
        "    negative_trigram_freq = [(word, negative_trigram_counts[0, idx]) for word, idx in ngram_vectorizer_neg.vocabulary_.items()]\n",
        "    negative_trigram_freq = sorted(negative_trigram_freq, key=lambda x: x[1], reverse=True)\n",
        "    negative_trigram_dict = dict(negative_trigram_freq)\n",
        "\n",
        "\n",
        "# Generate Word Cloud for Positive Trigrams (only if dictionary is not empty)\n",
        "if positive_trigram_dict:\n",
        "    wordcloud_positive_trigrams = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(positive_trigram_dict)\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.imshow(wordcloud_positive_trigrams, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud for Positive Trigrams (Three-Word Phrases)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No positive trigram word cloud generated due to empty data.\")\n",
        "\n",
        "\n",
        "# Generate Word Cloud for Negative Trigrams (only if dictionary is not empty)\n",
        "if negative_trigram_dict:\n",
        "    wordcloud_negative_trigrams = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate_from_frequencies(negative_trigram_dict)\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.imshow(wordcloud_negative_trigrams, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud for Negative Trigrams (Three-Word Phrases)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No negative trigram word cloud generated due to empty data.\")\n",
        "\n",
        "\n",
        "print(\"\\nWord clouds for trigrams generated (if data was available).\")\n",
        "print(\"Observe the difference in common phrases between positive and negative reviews.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbJKmdpzWPTf"
      },
      "outputs": [],
      "source": [
        "# --- Check for GPU availability ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sISBLP56KZaj"
      },
      "source": [
        "# --- BERT training (Logistic regression) --- #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YwALf6V7WdMQ"
      },
      "outputs": [],
      "source": [
        "file_path = 'Hotel_Reviews.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "    print(\"DataFrame loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found.\")\n",
        "    print(\"Please ensure you unzipped the file correctly and it's in the current directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the DataFrame: {e}\")\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "df['Positive_Review_Clean'] = df['Positive_Review'].astype(str).replace('No Positive', '')\n",
        "df['Negative_Review_Clean'] = df['Negative_Review'].astype(str).replace('No Negative', '')\n",
        "df['Review_Text'] = df['Positive_Review_Clean'] + ' ' + df['Negative_Review_Clean']\n",
        "df['Review_Text'] = df['Review_Text'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "df.drop(columns=['Positive_Review_Clean', 'Negative_Review_Clean'], inplace=True)\n",
        "df['Sentiment'] = df['Reviewer_Score'].apply(lambda score: 'Positive' if score >= 7 else 'Negative')\n",
        "\n",
        "print(f\"Total reviews after initial cleaning: {df.shape[0]}\")\n",
        "\n",
        "\n",
        "# --- USE THE FULL 500K+ DATASET FOR EMBEDDING GENERATION ---\n",
        "\n",
        "df_full_data_for_bert = df.copy() # <<<--- Use a copy of the full DataFrame\n",
        "\n",
        "# --- Check for GPU availability ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "\n",
        "# --- Load pre-trained BERT model and tokenizer ---\n",
        "print(\"\\nLoading BERT tokenizer and model...\")\n",
        "from transformers import AutoTokenizer, AutoModel # Added AutoModel import\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"BERT tokenizer and model loaded.\")\n",
        "\n",
        "\n",
        "# --- Function to generate BERT embeddings in batches ---\n",
        "def get_bert_embeddings(texts, tokenizer, model, device, batch_size=32, max_length=128):\n",
        "    all_embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating BERT Embeddings\"):\n",
        "        batch_texts = texts[i:min(i+batch_size, len(texts))]\n",
        "\n",
        "        encoded_input = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model_output = model(**encoded_input)\n",
        "\n",
        "        sentence_embeddings = model_output.pooler_output\n",
        "        all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "print(f\"\\nStarting BERT embedding generation on the FULL dataset of {df_full_data_for_bert.shape[0]} reviews.\")\n",
        "\n",
        "# Generate embeddings from the 'Review_Text' column of the FULL dataset\n",
        "bert_embeddings_full_data = get_bert_embeddings(\n",
        "    df_full_data_for_bert['Review_Text'].tolist(), # <<<--- Using df_full_data_for_bert\n",
        "    tokenizer,\n",
        "    model,\n",
        "    device,\n",
        "    batch_size=64,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "print(f\"\\nShape of generated BERT embeddings (full data): {bert_embeddings_full_data.shape}\")\n",
        "print(\"BERT embedding generation complete for full dataset.\")\n",
        "\n",
        "\n",
        "# --- ASSIGN X_BERT_FULL AND Y_BERT_FULL FOR TRAINING ---\n",
        "X_bert_full = bert_embeddings_full_data\n",
        "y_bert_full = df_full_data_for_bert['Sentiment'] # <<<--- Using sentiment from df_full_data_for_bert\n",
        "\n",
        "print(f\"\\nShape of features (X_bert_full): {X_bert_full.shape}\")\n",
        "print(f\"Shape of target (y_bert_full): {y_bert_full.shape}\")\n",
        "\n",
        "\n",
        "# --- SAVE THE BERT EMBEDDINGS AND PROCESSED FULL DATAFRAME ---\n",
        "processed_df_path_full_data = 'hotel_reviews_processed_full_data.parquet'\n",
        "bert_embeddings_path_full_data = 'bert_embeddings_full_data.npy'\n",
        "\n",
        "print(f\"\\nSaving processed DataFrame (full data) to {processed_df_path_full_data}...\")\n",
        "df_full_data_for_bert.to_parquet(processed_df_path_full_data, index=False) # Saving the full data df\n",
        "print(\"Processed DataFrame saved successfully!\")\n",
        "\n",
        "print(f\"Saving BERT embeddings (full data) to {bert_embeddings_path_full_data}...\")\n",
        "np.save(bert_embeddings_path_full_data, X_bert_full) # Save the assigned X_bert_full\n",
        "print(\"BERT embeddings saved successfully!\")\n",
        "\n",
        "print(\"\\nRemember to download these files from the Colab 'Files' tab if you want to store them locally.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pup-hupaTFX1"
      },
      "source": [
        "#--- Balancing the class weight positive/negative --- & DistilBERT fine tunning ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckChLRk9Ima4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "import gc\n",
        "import warnings\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoModel\n",
        "from datasets import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "# --- Configuration ---\n",
        "PROCESSED_DF_PATH = 'hotel_reviews_processed_full_data.parquet'\n",
        "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
        "MAX_LENGTH = 128\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 32\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "OUTPUT_DIR = \"./results_balanced_distilbert\"\n",
        "MODEL_SAVE_PATH = \"./balanced_distilbert_hotel_reviews\"\n",
        "\n",
        "# --- NLTK Downloads and Functions for Augmentation (needed for the training split) ---\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def synonym_replacement(text, n_replacements=2):\n",
        "    new_words = str(text).split()\n",
        "    random_word_list = list(set([word for word in new_words if word.lower() not in stop_words]))\n",
        "    if not random_word_list: return text\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = [lemma.name().replace(\"_\", \" \") for syn in wordnet.synsets(random_word) for lemma in syn.lemmas() if lemma.name().lower() != random_word.lower() and lemma.name().lower() not in stop_words]\n",
        "        if len(synonyms) > 0:\n",
        "            synonym = random.choice(synonyms)\n",
        "            try:\n",
        "                idx_to_replace = new_words.index(random_word)\n",
        "                new_words[idx_to_replace] = synonym\n",
        "                num_replaced += 1\n",
        "            except ValueError: pass\n",
        "        if num_replaced >= n_replacements: break\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# --- 1. Load the Original Processed Dataset ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "print(f\"Loading original dataset from '{PROCESSED_DF_PATH}'...\")\n",
        "try:\n",
        "    df = pd.read_parquet(PROCESSED_DF_PATH)\n",
        "    print(f\"DataFrame loaded successfully! Shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{PROCESSED_DF_PATH}' was not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Create Labels and Split into Unmodified Train/Test Sets ---\n",
        "print(\"\\nCreating labels and splitting data into original train/test sets...\")\n",
        "df['labels'] = df['Reviewer_Score'].apply(lambda score: 1 if score >= 7 else 0)\n",
        "train_df_original, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['labels'])\n",
        "print(f\"Original training set size: {len(train_df_original)}\")\n",
        "print(f\"Original test set size (unmodified): {len(test_df)}\")\n",
        "\n",
        "# --- 3. Augment the TRAINING DATA ONLY ---\n",
        "print(\"\\nAugmenting the training data...\")\n",
        "minority_df_train = train_df_original[train_df_original['labels'] == 0].copy()\n",
        "majority_count_train = len(train_df_original[train_df_original['labels'] == 1])\n",
        "minority_count_train = len(minority_df_train)\n",
        "num_to_augment = majority_count_train - minority_count_train\n",
        "\n",
        "if num_to_augment > 0:\n",
        "    reviews_to_augment = minority_df_train['Review_Text'].tolist()\n",
        "    augmented_reviews = []\n",
        "    for _ in tqdm(range(num_to_augment), desc=\"Augmenting reviews\"):\n",
        "        augmented_reviews.append(synonym_replacement(random.choice(reviews_to_augment)))\n",
        "\n",
        "    augmented_df = pd.DataFrame({'Review_Text': augmented_reviews, 'labels': [0] * len(augmented_reviews)})\n",
        "    train_df_augmented = pd.concat([train_df_original, augmented_df], ignore_index=True)\n",
        "    print(f\"Training data augmented. New training set size: {len(train_df_augmented)}\")\n",
        "else:\n",
        "    train_df_augmented = train_df_original.copy()\n",
        "    print(\"No augmentation needed.\")\n",
        "\n",
        "# Split augmented training data into training and validation sets\n",
        "train_df, val_df = train_test_split(train_df_augmented, test_size=0.1 / 0.9, random_state=42, stratify=train_df_augmented['labels'])\n",
        "print(f\"Final training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "del df, train_df_original, train_df_augmented, minority_df_train\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 4. Tokenization and Model Training (as before, but with corrected datasets) ---\n",
        "print(f\"\\nLoading tokenizer '{MODEL_CHECKPOINT}'...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        [str(text) if pd.notna(text) else \"\" for text in examples[\"Review_Text\"]],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "print(\"\\nTokenizing datasets...\")\n",
        "tokenized_train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "del train_df, val_df, test_df\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKOymWWAJRSS"
      },
      "outputs": [],
      "source": [
        "# --- 5. Define Metrics ---\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)\n",
        "    return {'accuracy': acc, 'f1_macro': f1, 'precision_macro': precision, 'recall_macro': recall}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8-XNnuwD76f"
      },
      "outputs": [],
      "source": [
        "# --- 6. Load the Pre-trained Model ---\n",
        "print(f\"\\nLoading '{MODEL_CHECKPOINT}' model for sequence classification...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
        "model.to(device)\n",
        "print(\"Model loaded and moved to device.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcvHJCMkJ7C5"
      },
      "outputs": [],
      "source": [
        "# --- 7. Define Training Arguments ---\n",
        "print(\"\\nSetting up TrainingArguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "print(\"TrainingArguments configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRPIhALB7dvJ"
      },
      "outputs": [],
      "source": [
        "# --- 8. Initialize and Run the Trainer ---\n",
        "print(\"\\nInitializing Trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "print(\"Starting model fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4aMGQNBEOia"
      },
      "outputs": [],
      "source": [
        "# --- 9. Evaluate on the Unseen Test Set ---\n",
        "print(\"\\n--- Evaluating on the unseen Test Set ---\")\n",
        "test_results = trainer.evaluate(tokenized_test_dataset)\n",
        "print(f\"Test Set Evaluation Results: {test_results}\")\n",
        "\n",
        "predictions_output = trainer.predict(tokenized_test_dataset)\n",
        "y_pred_logits = predictions_output.predictions\n",
        "y_pred_labels = np.argmax(y_pred_logits, axis=-1)\n",
        "y_true_labels = predictions_output.label_ids\n",
        "\n",
        "print(\"\\n--- Full Classification Report on Test Set (Fine-tuned DistilBERT) ---\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=['Negative', 'Positive']))\n",
        "\n",
        "cm = confusion_matrix(y_true_labels, y_pred_labels, labels=[0, 1])\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix on Test Set (Fine-tuned DistilBERT)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96UkmBNxESZe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- ADD THIS LINE ---\n",
        "RESULTS_DIR = \"./evaluation_results_balanced_distilbert\"\n",
        "# --- END ADDITION ---\n",
        "\n",
        "print(f\"\\nSaving fine-tuned model and tokenizer to: {MODEL_SAVE_PATH}\")\n",
        "trainer.save_model(MODEL_SAVE_PATH)\n",
        "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
        "print(\"Fine-tuned model and tokenizer saved successfully!\")\n",
        "\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "final_metrics_json = {\n",
        "    \"test_results\": test_results,\n",
        "    \"classification_report\": classification_report(y_true_labels, y_pred_labels, target_names=['Negative', 'Positive'], output_dict=True, zero_division=0),\n",
        "    \"model_name\": MODEL_CHECKPOINT,\n",
        "    \"epochs\": NUM_TRAIN_EPOCHS,\n",
        "    \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"max_length\": MAX_LENGTH,\n",
        "    \"timestamp\": pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "}\n",
        "json_filename = os.path.join(RESULTS_DIR, f\"balanced_distilbert_metrics_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "with open(json_filename, 'w') as f:\n",
        "    json.dump(final_metrics_json, f, indent=4)\n",
        "print(f\"\\nNumerical metrics (JSON) saved to: {json_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RabtKYC1EX4e"
      },
      "outputs": [],
      "source": [
        "# --- 11. Inference on New Unseen Data (using the fine-tuned model) ---\n",
        "print(\"\\n--- Starting inference on new data using the fine-tuned model ---\")\n",
        "\n",
        "# Assuming the following variables are already in memory from previous cells:\n",
        "# MODEL_SAVE_PATH, NEW_DATA_URL, MAX_LENGTH, device\n",
        "\n",
        "print(f\"Loading fine-tuned model and tokenizer from '{MODEL_SAVE_PATH}'...\")\n",
        "try:\n",
        "    fine_tuned_tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
        "    fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
        "    fine_tuned_model.to(device)\n",
        "    fine_tuned_model.eval()\n",
        "    print(\"Fine-tuned model loaded successfully for inference.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\nLoading new data from URL: {NEW_DATA_URL}\")\n",
        "try:\n",
        "    new_df = pd.read_csv(NEW_DATA_URL)\n",
        "    new_df['Review_Text'] = new_df['Review_Text'].astype(str)\n",
        "    print(f\"New data loaded successfully! Found {new_df.shape[0]} reviews.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading new data: {e}\")\n",
        "    exit()\n",
        "\n",
        "INFERENCE_BATCH_SIZE = 64\n",
        "print(\"\\nMaking predictions on the new data in batches...\")\n",
        "predictions = []\n",
        "review_texts = new_df['Review_Text'].tolist()\n",
        "\n",
        "for i in tqdm(range(0, len(review_texts), INFERENCE_BATCH_SIZE), desc=\"Predicting in batches\"):\n",
        "    batch_texts = review_texts[i:i + INFERENCE_BATCH_SIZE]\n",
        "\n",
        "    tokenized_batch = fine_tuned_tokenizer(\n",
        "        batch_texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = fine_tuned_model(**tokenized_batch)\n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        predictions.extend(batch_predictions)\n",
        "\n",
        "    del tokenized_batch, outputs, logits, batch_predictions\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "new_df['Predicted_Sentiment'] = predictions\n",
        "new_df['Predicted_Sentiment'] = new_df['Predicted_Sentiment'].map({1: 'Positive', 0: 'Negative'})\n",
        "print(\"Predictions made and added to the DataFrame.\")\n",
        "\n",
        "print(\"\\n--- Final Predictions on New Data ---\")\n",
        "print(new_df[['Review_Text', 'Predicted_Sentiment']].head(10))\n",
        "print(\"\\nPrediction distribution:\")\n",
        "print(new_df['Predicted_Sentiment'].value_counts())\n",
        "\n",
        "output_file_path = 'hotel_sentiment_predictions_balanced_distilbert.csv'\n",
        "print(f\"\\nSaving results to {output_file_path}...\")\n",
        "new_df.to_csv(output_file_path, index=False)\n",
        "print(\"Results saved successfully.\")\n",
        "\n",
        "try:\n",
        "    files.download(output_file_path)\n",
        "    print(\"\\nDownload initiated. Check your browser for the file.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during file download: {e}\")\n",
        "    print(\"You can manually download the file from the Colab file browser (folder icon on the left).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reF6kEz5Mx-S"
      },
      "source": [
        "# **# Fine Tunning with RoBERTa #**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CKFTOon6dkz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "import gc\n",
        "import warnings\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "# --- Configuration for RoBERTa Fine-Tuning ---\n",
        "PROCESSED_DF_PATH = 'hotel_reviews_processed_full_data.parquet'\n",
        "MODEL_CHECKPOINT = \"roberta-base\" # The new model to fine-tune\n",
        "MAX_LENGTH = 128\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 32\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "OUTPUT_DIR = \"./results_roberta\"\n",
        "MODEL_SAVE_PATH = \"./roberta_hotel_reviews\"\n",
        "RESULTS_DIR = \"./evaluation_results_roberta\"\n",
        "\n",
        "# --- NLTK Downloads and Functions for Augmentation (needed for the training split) ---\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def synonym_replacement(text, n_replacements=2):\n",
        "    new_words = str(text).split()\n",
        "    random_word_list = list(set([word for word in new_words if word.lower() not in stop_words]))\n",
        "    if not random_word_list: return text\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = [lemma.name().replace(\"_\", \" \") for syn in wordnet.synsets(random_word) for lemma in syn.lemmas() if lemma.name().lower() != random_word.lower() and lemma.name().lower() not in stop_words]\n",
        "        if len(synonyms) > 0:\n",
        "            synonym = random.choice(synonyms)\n",
        "            try:\n",
        "                idx_to_replace = new_words.index(random_word)\n",
        "                new_words[idx_to_replace] = synonym\n",
        "                num_replaced += 1\n",
        "            except ValueError: pass\n",
        "        if num_replaced >= n_replacements: break\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# --- 1. Load the Original Processed Dataset ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "print(f\"Loading original dataset from '{PROCESSED_DF_PATH}'...\")\n",
        "try:\n",
        "    df = pd.read_parquet(PROCESSED_DF_PATH)\n",
        "    print(f\"DataFrame loaded successfully! Shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{PROCESSED_DF_PATH}' was not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Create Labels and Split into Unmodified Train/Test Sets ---\n",
        "print(\"\\nCreating labels and splitting data into original train/test sets...\")\n",
        "df['labels'] = df['Reviewer_Score'].apply(lambda score: 1 if score >= 7 else 0)\n",
        "train_df_original, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['labels'])\n",
        "print(f\"Original training set size: {len(train_df_original)}\")\n",
        "print(f\"Original test set size (unmodified): {len(test_df)}\")\n",
        "\n",
        "# --- 3. Augment the TRAINING DATA ONLY ---\n",
        "print(\"\\nAugmenting the training data...\")\n",
        "minority_df_train = train_df_original[train_df_original['labels'] == 0].copy()\n",
        "majority_count_train = len(train_df_original[train_df_original['labels'] == 1])\n",
        "minority_count_train = len(minority_df_train)\n",
        "num_to_augment = majority_count_train - minority_count_train\n",
        "\n",
        "if num_to_augment > 0:\n",
        "    reviews_to_augment = minority_df_train['Review_Text'].tolist()\n",
        "    augmented_reviews = []\n",
        "    for _ in tqdm(range(num_to_augment), desc=\"Augmenting reviews\"):\n",
        "        augmented_reviews.append(synonym_replacement(random.choice(reviews_to_augment)))\n",
        "\n",
        "    augmented_df = pd.DataFrame({'Review_Text': augmented_reviews, 'labels': [0] * len(augmented_reviews)})\n",
        "    train_df_augmented = pd.concat([train_df_original, augmented_df], ignore_index=True)\n",
        "    print(f\"Training data augmented. New training set size: {len(train_df_augmented)}\")\n",
        "else:\n",
        "    train_df_augmented = train_df_original.copy()\n",
        "    print(\"No augmentation needed.\")\n",
        "\n",
        "train_df, val_df = train_test_split(train_df_augmented, test_size=0.1 / 0.9, random_state=42, stratify=train_df_augmented['labels'])\n",
        "print(f\"Final training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "del df, train_df_original, train_df_augmented, minority_df_train\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 4. Tokenization ---\n",
        "print(f\"\\nLoading tokenizer '{MODEL_CHECKPOINT}'...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        [str(text) if pd.notna(text) else \"\" for text in examples[\"Review_Text\"]],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "print(\"\\nTokenizing datasets...\")\n",
        "tokenized_train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "del train_df, val_df, test_df\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 5. Define Metrics ---\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)\n",
        "    return {'accuracy': acc, 'f1_macro': f1, 'precision_macro': precision, 'recall_macro': recall}\n",
        "\n",
        "# --- 6. Load the Pre-trained Model ---\n",
        "print(f\"\\nLoading '{MODEL_CHECKPOINT}' model for sequence classification...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
        "model.to(device)\n",
        "print(\"Model loaded and moved to device.\")\n",
        "\n",
        "# --- 7. Define Training Arguments ---\n",
        "print(\"\\nSetting up TrainingArguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "print(\"TrainingArguments configured.\")\n",
        "\n",
        "# --- 8. Initialize and Run the Trainer ---\n",
        "print(\"\\nInitializing Trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "print(\"Starting model fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete.\")\n",
        "\n",
        "# --- 9. Evaluate on the Unseen Test Set ---\n",
        "print(\"\\n--- Evaluating on the unseen Test Set ---\")\n",
        "test_results = trainer.evaluate(tokenized_test_dataset)\n",
        "print(f\"Test Set Evaluation Results: {test_results}\")\n",
        "\n",
        "predictions_output = trainer.predict(tokenized_test_dataset)\n",
        "y_pred_logits = predictions_output.predictions\n",
        "y_pred_labels = np.argmax(y_pred_logits, axis=-1)\n",
        "y_true_labels = predictions_output.label_ids\n",
        "\n",
        "print(\"\\n--- Full Classification Report on Test Set (Fine-tuned RoBERTa) ---\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=['Negative', 'Positive']))\n",
        "\n",
        "cm = confusion_matrix(y_true_labels, y_pred_labels, labels=[0, 1])\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix on Test Set (Fine-tuned RoBERTa)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# --- 10. Save the Fine-tuned Model and Evaluation Metrics ---\n",
        "print(f\"\\nSaving fine-tuned model and tokenizer to: {MODEL_SAVE_PATH}\")\n",
        "trainer.save_model(MODEL_SAVE_PATH)\n",
        "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
        "print(\"Fine-tuned model and tokenizer saved successfully!\")\n",
        "\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "final_metrics_json = {\n",
        "    \"test_results\": test_results,\n",
        "    \"classification_report\": classification_report(y_true_labels, y_pred_labels, target_names=['Negative', 'Positive'], output_dict=True, zero_division=0),\n",
        "    \"model_name\": MODEL_CHECKPOINT,\n",
        "    \"epochs\": NUM_TRAIN_EPOCHS,\n",
        "    \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"max_length\": MAX_LENGTH,\n",
        "    \"timestamp\": pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "}\n",
        "json_filename = os.path.join(RESULTS_DIR, f\"roberta_metrics_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "with open(json_filename, 'w') as f:\n",
        "    json.dump(final_metrics_json, f, indent=4)\n",
        "print(f\"\\nNumerical metrics (JSON) saved to: {json_filename}\")\n",
        "\n",
        "# --- 11. Inference on New Unseen Data (using the fine-tuned model) ---\n",
        "print(\"\\n--- Starting inference on new data using the fine-tuned model ---\")\n",
        "# This is a separate block for later, as the fine-tuning process takes time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezrJjbvJ3-UW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "models_to_download = [\n",
        "    \"bert-base-uncased\",\n",
        "    \"distilbert-base-uncased\",\n",
        "    \"roberta-base\"\n",
        "]\n",
        "\n",
        "for model_name in tqdm(models_to_download, desc=\"Downloading models\"):\n",
        "    print(f\"\\n--- Downloading and loading tokenizer and model for {model_name} ---\")\n",
        "    try:\n",
        "        # Download and cache the tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        print(\"Tokenizer downloaded.\")\n",
        "\n",
        "        # Download and cache the model\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "        print(\"Model downloaded.\")\n",
        "\n",
        "        print(f\"✅ Download for {model_name} complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading {model_name}: {e}\")\n",
        "\n",
        "print(\"\\nAll models have been downloaded and cached successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2klnipX5Pk"
      },
      "source": [
        "#--- Compare the Performance of ALL Models ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za6WlunHNBPU"
      },
      "source": [
        "This is the TEST dataset we have\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/e/2PACX-1vSQOXCs_XKH1DCvzmo0RdGmm4tfT_sKRCsCKBIpkMqdRq2xKHG_61Rtz7_S7cA_O5mmlng1sd5jgoeR/pub?output=csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "import gc\n",
        "import warnings\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "from datasets import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files # For downloading the CSV\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub')\n",
        "\n",
        "# --- Configuration ---\n",
        "PROCESSED_DF_PATH = 'hotel_reviews_processed_full_data.parquet' # Original data for test set\n",
        "NEW_DATA_URL = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSQOXCs_XKH1DCvzmo0RdGmm4tfT_sKRCsCKBIpkMqdRq2xKHG_61Rtz7_S7cA_O5mmlng1sd5jgoeR/pub?output=csv\"\n",
        "\n",
        "BERT_LOGREG_FULL_MODEL_PATH = \"logistic_regression_model_bert_embeddings_full_data.joblib\"\n",
        "BERT_LOGREG_BALANCED_MODEL_PATH = \"logistic_regression_model_bert_embeddings_balanced_data.joblib\"\n",
        "DISTILBERT_MODEL_DIR = \"./balanced_distilbert_hotel_reviews\"\n",
        "ROBERTA_MODEL_DIR = \"./roberta_hotel_reviews\"\n",
        "\n",
        "BERT_BASE_MODEL_NAME = \"bert-base-uncased\"\n",
        "DISTILBERT_BASE_MODEL_NAME = \"distilbert-base-uncased\"\n",
        "ROBERTA_BASE_MODEL_NAME = \"roberta-base\"\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "INFERENCE_BATCH_SIZE = 8 # Keep this small for transformer inference to avoid OOM\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "# --- Helper Functions (defined once) ---\n",
        "\n",
        "def get_bert_embeddings(texts, tokenizer_name, model_name, device, batch_size=INFERENCE_BATCH_SIZE, max_length=MAX_LENGTH):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    string_texts = [str(t) for t in texts]\n",
        "    for i in tqdm(range(0, len(string_texts), batch_size), desc=\"Generating BERT Embeddings\"):\n",
        "        batch_texts = string_texts[i:min(i + batch_size, len(string_texts))]\n",
        "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt').to(device)\n",
        "        with torch.no_grad():\n",
        "            model_output = model(**encoded_input)\n",
        "        sentence_embeddings = model_output.pooler_output\n",
        "        all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
        "    del tokenizer, model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "def predict_with_transformer(model_path, texts, base_tokenizer_name, device, batch_size=INFERENCE_BATCH_SIZE, max_length=MAX_LENGTH, return_probabilities=False):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_probabilities = [] if return_probabilities else None\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Predicting with {os.path.basename(model_path)}\"):\n",
        "        batch_texts = texts[i:min(i + batch_size, len(texts))]\n",
        "        tokenized_batch = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**tokenized_batch)\n",
        "            logits = outputs.logits\n",
        "            batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            all_predictions.extend(batch_preds)\n",
        "            if return_probabilities:\n",
        "                batch_probs = torch.softmax(logits, dim=-1).cpu().numpy()[:, 1] # Prob of positive class\n",
        "                all_probabilities.extend(batch_probs)\n",
        "        del tokenized_batch, outputs, logits\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    del tokenizer, model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if return_probabilities:\n",
        "        return all_predictions, all_probabilities\n",
        "    else:\n",
        "        return all_predictions\n",
        "\n",
        "def get_metrics_for_model(model_name, y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'], output_dict=True, zero_division=0)\n",
        "    return {\n",
        "        \"Modelo\": model_name,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"F1-Macro\": report['macro avg']['f1-score'],\n",
        "        \"Precision-Macro\": report['macro avg']['precision'],\n",
        "        \"Recall-Macro\": report['macro avg']['recall'],\n",
        "        \"Positive Precision\": report['Positive']['precision'],\n",
        "        \"Positive Recall\": report['Positive']['recall'],\n",
        "        \"Positive F1\": report['Positive']['f1-score'],\n",
        "        \"Negative Precision\": report['Negative']['precision'],\n",
        "        \"Negative Recall\": report['Negative']['recall'],\n",
        "        \"Negative F1\": report['Negative']['f1-score']\n",
        "    }\n",
        "\n",
        "# ==============================================================================\n",
        "## Part 1: Theoretical Evaluation on `test_df` (with metrics)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"== Part 1: Theoretical Evaluation on Original Test Set (with metrics) ==\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Load the Original, Untouched Test Set ---\n",
        "print(f\"Loading original processed data from '{PROCESSED_DF_PATH}' to get the test set...\")\n",
        "try:\n",
        "    df_original = pd.read_parquet(PROCESSED_DF_PATH)\n",
        "    df_original['labels'] = df_original['Reviewer_Score'].apply(lambda score: 1 if score >= 7 else 0)\n",
        "\n",
        "    _, test_df_original = train_test_split(df_original, test_size=0.1, random_state=42, stratify=df_original['labels'])\n",
        "\n",
        "    print(f\"Original test set loaded. Size: {len(test_df_original)}\")\n",
        "    y_true_test_original = test_df_original['labels'].tolist() # True labels for evaluation\n",
        "\n",
        "    del df_original\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{PROCESSED_DF_PATH}' was not found. Cannot perform theoretical evaluation.\")\n",
        "    test_df_original = pd.DataFrame({'Review_Text': [], 'labels': []}) # Create empty df to avoid errors\n",
        "    y_true_test_original = []\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during test set loading for theoretical evaluation: {e}\")\n",
        "    test_df_original = pd.DataFrame({'Review_Text': [], 'labels': []})\n",
        "    y_true_test_original = []\n",
        "\n",
        "\n",
        "# --- List to store results for each model's theoretical performance ---\n",
        "all_theoretical_metrics = []\n",
        "\n",
        "# --- Evaluate Each Model on the Original Test Set ---\n",
        "\n",
        "# BERT (Logistic Regression) on Imbalanced Data\n",
        "print(\"\\n--- Evaluating BERT (Logistic Regression) on Imbalanced Data ---\")\n",
        "if not test_df_original.empty:\n",
        "    try:\n",
        "        log_reg_full_model = joblib.load(BERT_LOGREG_FULL_MODEL_PATH)\n",
        "        test_embeddings_bert_full = get_bert_embeddings(test_df_original['Review_Text'].tolist(), BERT_BASE_MODEL_NAME, BERT_BASE_MODEL_NAME, device)\n",
        "        y_pred_bert_full_str = log_reg_full_model.predict(test_embeddings_bert_full)\n",
        "        y_pred_bert_full = np.array([1 if p == 'Positive' else 0 for p in y_pred_bert_full_str])\n",
        "        all_theoretical_metrics.append(get_metrics_for_model(\"BERT (LogReg) - Imbalanced Data\", y_true_test_original, y_pred_bert_full))\n",
        "        del log_reg_full_model, test_embeddings_bert_full, y_pred_bert_full_str, y_pred_bert_full\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: BERT LogReg (Imbalanced) model not found at {BERT_LOGREG_FULL_MODEL_PATH}. Skipping theoretical evaluation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating BERT LogReg (Imbalanced) model: {e}. Skipping theoretical evaluation.\")\n",
        "\n",
        "# BERT (Logistic Regression) on Balanced Data\n",
        "print(\"\\n--- Evaluating BERT (Logistic Regression) on Balanced Data ---\")\n",
        "if not test_df_original.empty:\n",
        "    try:\n",
        "        log_reg_balanced_model = joblib.load(BERT_LOGREG_BALANCED_MODEL_PATH)\n",
        "        test_embeddings_bert_balanced = get_bert_embeddings(test_df_original['Review_Text'].tolist(), BERT_BASE_MODEL_NAME, BERT_BASE_MODEL_NAME, device)\n",
        "        y_pred_bert_balanced_str = log_reg_balanced_model.predict(test_embeddings_bert_balanced)\n",
        "        y_pred_bert_balanced = np.array([1 if p == 'Positive' else 0 for p in y_pred_bert_balanced_str])\n",
        "        all_theoretical_metrics.append(get_metrics_for_model(\"BERT (LogReg) - Balanced Data\", y_true_test_original, y_pred_bert_balanced))\n",
        "        del log_reg_balanced_model, test_embeddings_bert_balanced, y_pred_bert_balanced_str, y_pred_bert_balanced\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: BERT LogReg (Balanced) model not found at {BERT_LOGREG_BALANCED_MODEL_PATH}. Skipping theoretical evaluation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating BERT LogReg (Balanced) model: {e}. Skipping theoretical evaluation.\")\n",
        "\n",
        "# Fine-tuned DistilBERT\n",
        "print(\"\\n--- Evaluating Fine-tuned DistilBERT ---\")\n",
        "if not test_df_original.empty:\n",
        "    try:\n",
        "        y_pred_distilbert = predict_with_transformer(DISTILBERT_MODEL_DIR, test_df_original['Review_Text'].tolist(), DISTILBERT_BASE_MODEL_NAME, device)\n",
        "        all_theoretical_metrics.append(get_metrics_for_model(\"DistilBERT (Fine-tuned)\", y_true_test_original, y_pred_distilbert))\n",
        "        del y_pred_distilbert\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: DistilBERT model not found at {DISTILBERT_MODEL_DIR}. Skipping theoretical evaluation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating DistilBERT model: {e}. Skipping theoretical evaluation.\")\n",
        "\n",
        "# Fine-tuned RoBERTa\n",
        "print(\"\\n--- Evaluating Fine-tuned RoBERTa ---\")\n",
        "if not test_df_original.empty:\n",
        "    try:\n",
        "        y_pred_roberta = predict_with_transformer(ROBERTA_MODEL_DIR, test_df_original['Review_Text'].tolist(), ROBERTA_BASE_MODEL_NAME, device)\n",
        "        all_theoretical_metrics.append(get_metrics_for_model(\"RoBERTa (Fine-tuned)\", y_true_test_original, y_pred_roberta))\n",
        "        del y_pred_roberta\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: RoBERTa model not found at {ROBERTA_MODEL_DIR}. Skipping theoretical evaluation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating RoBERTa model: {e}. Skipping theoretical evaluation.\")\n",
        "\n",
        "# --- Display Theoretical Comparison Table ---\n",
        "print(\"\\n--- Theoretical Model Performance Comparison Table ---\")\n",
        "if all_theoretical_metrics:\n",
        "    theoretical_comparison_df = pd.DataFrame(all_theoretical_metrics)\n",
        "    ordered_cols = [\n",
        "        \"Modelo\", \"Accuracy\", \"F1-Macro\", \"Precision-Macro\", \"Recall-Macro\",\n",
        "        \"Positive Precision\", \"Positive Recall\", \"Positive F1\",\n",
        "        \"Negative Precision\", \"Negative Recall\", \"Negative F1\"\n",
        "    ]\n",
        "    theoretical_comparison_df = theoretical_comparison_df[ordered_cols]\n",
        "    for col in theoretical_comparison_df.columns:\n",
        "        if col not in [\"Modelo\"]:\n",
        "            theoretical_comparison_df[col] = theoretical_comparison_df[col].apply(lambda x: f\"{x:.4f}\")\n",
        "    print(theoretical_comparison_df.to_string(index=False))\n",
        "    theoretical_comparison_csv_path = \"theoretical_model_performance_comparison.csv\"\n",
        "    theoretical_comparison_df.to_csv(theoretical_comparison_csv_path, index=False)\n",
        "    print(f\"\\nTheoretical comparison table saved to {theoretical_comparison_csv_path}\")\n",
        "    try:\n",
        "        files.download(theoretical_comparison_csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading theoretical comparison CSV: {e}\")\n",
        "else:\n",
        "    print(\"No models were successfully evaluated for theoretical comparison.\")\n",
        "\n",
        "# ==============================================================================\n",
        "## Part 2: Real-Life Prediction Comparison on `new_df` (without metrics)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"== Part 2: Real-Life Prediction Comparison on New Unseen Data ==\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Load the New Data for Prediction ---\n",
        "print(f\"Loading new data for prediction from URL: {NEW_DATA_URL}...\")\n",
        "try:\n",
        "    new_df_real_life = pd.read_csv(NEW_DATA_URL)\n",
        "    new_df_real_life['Review_Text'] = new_df_real_life['Review_Text'].astype(str).str.strip()\n",
        "    print(f\"New data loaded successfully! Size: {len(new_df_real_life)} reviews.\")\n",
        "    print(\"\\nWARNING: This dataset does NOT contain ground truth labels. No performance metrics (Accuracy, F1, etc.) can be calculated.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during new data loading for real-life comparison: {e}\")\n",
        "    new_df_real_life = pd.DataFrame({'Review_Text': []})\n",
        "\n",
        "# --- Make Predictions with each Model on the New Data ---\n",
        "\n",
        "# BERT (Logistic Regression) on Balanced Data\n",
        "print(\"\\n--- Making predictions with BERT (Logistic Regression) on Balanced Data ---\")\n",
        "if not new_df_real_life.empty:\n",
        "    try:\n",
        "        log_reg_balanced_model = joblib.load(BERT_LOGREG_BALANCED_MODEL_PATH)\n",
        "        test_embeddings_bert_balanced = get_bert_embeddings(new_df_real_life['Review_Text'].tolist(), BERT_BASE_MODEL_NAME, BERT_BASE_MODEL_NAME, device)\n",
        "\n",
        "        y_pred_bert_balanced_str = log_reg_balanced_model.predict(test_embeddings_bert_balanced)\n",
        "        y_prob_bert_balanced = log_reg_balanced_model.predict_proba(test_embeddings_bert_balanced)[:, 1] # Get probabilities\n",
        "\n",
        "        new_df_real_life['BERT_Predicted_Sentiment'] = y_pred_bert_balanced_str\n",
        "        new_df_real_life['BERT_Prob_Positive'] = y_prob_bert_balanced\n",
        "\n",
        "        del log_reg_balanced_model, test_embeddings_bert_balanced, y_pred_bert_balanced_str, y_prob_bert_balanced\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: BERT LogReg (Balanced) model not found at {BERT_LOGREG_BALANCED_MODEL_PATH}. Skipping real-life predictions.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error making real-life predictions with BERT LogReg (Balanced) model: {e}. Skipping predictions.\")\n",
        "\n",
        "# Fine-tuned DistilBERT\n",
        "print(\"\\n--- Making predictions with fine-tuned DistilBERT ---\")\n",
        "if not new_df_real_life.empty:\n",
        "    try:\n",
        "        y_pred_distilbert_int, y_prob_distilbert = predict_with_transformer(DISTILBERT_MODEL_DIR, new_df_real_life['Review_Text'].tolist(), DISTILBERT_BASE_MODEL_NAME, device, return_probabilities=True)\n",
        "\n",
        "        new_df_real_life['DistilBERT_Predicted_Sentiment'] = np.array([ 'Positive' if p == 1 else 'Negative' for p in y_pred_distilbert_int])\n",
        "        new_df_real_life['DistilBERT_Prob_Positive'] = y_prob_distilbert\n",
        "\n",
        "        del y_pred_distilbert_int, y_prob_distilbert\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: DistilBERT model not found at {DISTILBERT_MODEL_DIR}. Skipping real-life predictions.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error making real-life predictions with DistilBERT model: {e}. Skipping predictions.\")\n",
        "\n",
        "# Fine-tuned RoBERTa\n",
        "print(\"\\n--- Making predictions with fine-tuned RoBERTa ---\")\n",
        "if not new_df_real_life.empty:\n",
        "    try:\n",
        "        y_pred_roberta_int, y_prob_roberta = predict_with_transformer(ROBERTA_MODEL_DIR, new_df_real_life['Review_Text'].tolist(), ROBERTA_BASE_MODEL_NAME, device, return_probabilities=True)\n",
        "\n",
        "        new_df_real_life['RoBERTa_Predicted_Sentiment'] = np.array([ 'Positive' if p == 1 else 'Negative' for p in y_pred_roberta_int])\n",
        "        new_df_real_life['RoBERTa_Prob_Positive'] = y_prob_roberta\n",
        "\n",
        "        del y_pred_roberta_int, y_prob_roberta\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: RoBERTa model not found at {ROBERTA_MODEL_DIR}. Skipping real-life predictions.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error making real-life predictions with RoBERTa model: {e}. Skipping predictions.\")\n",
        "\n",
        "\n",
        "# --- Display Real-Life Prediction Comparison Table ---\n",
        "print(\"\\n--- Real-Life Model Prediction Comparison ---\")\n",
        "\n",
        "sentiment_cols_real_life = [col for col in new_df_real_life.columns if 'Predicted_Sentiment' in col]\n",
        "if 'Review_Text' in new_df_real_life.columns and sentiment_cols_real_life:\n",
        "    print(\"\\nPredicted Sentiments (First 20 Reviews):\")\n",
        "    print(new_df_real_life[['Review_Text'] + sentiment_cols_real_life].head(20).to_string(index=False))\n",
        "else:\n",
        "    print(\"No sentiment predictions available for real-life data.\")\n",
        "\n",
        "prob_cols_real_life = [col for col in new_df_real_life.columns if 'Prob_Positive' in col]\n",
        "if 'Review_Text' in new_df_real_life.columns and prob_cols_real_life:\n",
        "    print(\"\\nPredicted Probabilities of Positive Sentiment (First 20 Reviews):\")\n",
        "    print(new_df_real_life[['Review_Text'] + prob_cols_real_life].head(20).to_string(index=False))\n",
        "else:\n",
        "    print(\"No probability predictions available for real-life data.\")\n",
        "\n",
        "# Hard Voting\n",
        "available_sentiment_cols_real_life = [col for col in ['BERT_Predicted_Sentiment', 'DistilBERT_Predicted_Sentiment', 'RoBERTa_Predicted_Sentiment'] if col in new_df_real_life.columns]\n",
        "if len(available_sentiment_cols_real_life) >= 2:\n",
        "    temp_df_for_hard_vote = new_df_real_life[available_sentiment_cols_real_life].replace({'Positive': 1, 'Negative': 0})\n",
        "    hard_voting_predictions_numeric = temp_df_for_hard_vote.mode(axis=1)[0]\n",
        "    new_df_real_life['Hard_Voting_Prediction'] = hard_voting_predictions_numeric.map({1: 'Positive', 0: 'Negative'})\n",
        "    print(\"\\nHard Voting Predictions (First 20 Reviews):\")\n",
        "    print(new_df_real_life[['Review_Text', 'Hard_Voting_Prediction']].head(20).to_string(index=False))\n",
        "else:\n",
        "    print(\"\\nSkipping Hard Voting: Not enough model predictions available for real-life data.\")\n",
        "\n",
        "# Soft Voting\n",
        "available_prob_cols_real_life = [col for col in ['BERT_Prob_Positive', 'DistilBERT_Prob_Positive', 'RoBERTa_Prob_Positive'] if col in new_df_real_life.columns]\n",
        "if len(available_prob_cols_real_life) >= 2:\n",
        "    new_df_real_life['Soft_Voting_Avg_Prob_Positive'] = new_df_real_life[available_prob_cols_real_life].mean(axis=1)\n",
        "    new_df_real_life['Soft_Voting_Prediction'] = new_df_real_life['Soft_Voting_Avg_Prob_Positive'].apply(lambda x: 'Positive' if x > 0.5 else 'Negative')\n",
        "    print(\"\\nSoft Voting Predictions (First 20 Reviews):\")\n",
        "    print(new_df_real_life[['Review_Text', 'Soft_Voting_Prediction']].head(20).to_string(index=False))\n",
        "else:\n",
        "    print(\"\\nSkipping Soft Voting: Not enough model probabilities available for real-life data.\")\n",
        "\n",
        "\n",
        "print(\"\\nPrediction counts for each model and voting method (Real-Life Data):\")\n",
        "all_prediction_cols_real_life = sentiment_cols_real_life + ['Hard_Voting_Prediction', 'Soft_Voting_Prediction']\n",
        "for col in all_prediction_cols_real_life:\n",
        "    if col in new_df_real_life.columns:\n",
        "        print(f\"\\n{col}:\\n\", new_df_real_life[col].value_counts())\n",
        "\n",
        "output_file_path_real_life = 'real_life_predictions_comparison.csv'\n",
        "print(f\"\\nSaving real-life prediction comparison to '{output_file_path_real_life}'...\")\n",
        "new_df_real_life.to_csv(output_file_path_real_life, index=False)\n",
        "print(\"File saved successfully.\")\n",
        "\n",
        "try:\n",
        "    files.download(output_file_path_real_life)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading real-life comparison CSV: {e}\")\n",
        "    print(\"You can manually download the file from the Colab file browser.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"== Combined Evaluation Complete ==\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "wYO4bwuxiRYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#** STREAMLIT APP **#"
      ],
      "metadata": {
        "id": "R3bUpPAGw77O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok -qq"
      ],
      "metadata": {
        "id": "2lCPOhfdwAqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os # Import os to read environment variables\n",
        "import requests\n",
        "import time\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import io\n",
        "\n",
        "# --- NLTK Downloads (required for stopwords) ---\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# --- 0. OpenAI API Key Configuration ---\n",
        "# IMPORTANT: Read the API key from environment variables\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\") # Reads from env var, defaults to empty string if not found\n",
        "\n",
        "# --- 1. Configuration for your fine-tuned sentiment model ---\n",
        "MODEL_SAVE_PATH = \"./roberta_hotel_reviews\" # e.g., \"./balanced_distilbert_hotel_reviews\"\n",
        "\n",
        "# Set device for PyTorch models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- NEW: Minimum reviews threshold for dashboard display ---\n",
        "MIN_REVIEWS_FOR_DASHBOARD = 5\n",
        "\n",
        "# --- 2. Load your fine-tuned sentiment model and tokenizer (cached for performance) ---\n",
        "@st.cache_resource\n",
        "def load_sentiment_model():\n",
        "    \"\"\"Load the fine-tuned sentiment model and tokenizer.\"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        st.success(\"Sentiment model loaded successfully!\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading sentiment model: {e}\")\n",
        "        st.info(\"Please ensure the model directory exists and contains all necessary files.\")\n",
        "        return None, None\n",
        "\n",
        "sentiment_tokenizer, sentiment_model = load_sentiment_model()\n",
        "\n",
        "# --- 3. Define Sentiment Prediction Function (for a single review) ---\n",
        "def predict_sentiment_single(review_text):\n",
        "    \"\"\"Predicts sentiment for a single text using the loaded sentiment model.\"\"\"\n",
        "    if not sentiment_model or not sentiment_tokenizer:\n",
        "        return None, None\n",
        "\n",
        "    encoded_input = sentiment_tokenizer(\n",
        "        review_text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = sentiment_model(**encoded_input)\n",
        "        logits = outputs.logits\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        predicted_class_id = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    sentiment_map = {1: \"Positive\", 0: \"Negative\"}\n",
        "    predicted_sentiment = sentiment_map.get(predicted_class_id, \"Unknown\")\n",
        "    positive_probability = probabilities[0, 1].item()\n",
        "\n",
        "    return predicted_sentiment, positive_probability\n",
        "\n",
        "# --- 4. OpenAI Integration for Summarization and Thematic Insights (for a single review) ---\n",
        "def get_openai_insights_single(review_text):\n",
        "    \"\"\"\n",
        "    Calls OpenAI API to get summary, key points categorized by theme, and action items for a single review.\n",
        "    Uses a structured JSON response format.\n",
        "    \"\"\"\n",
        "    if not OPENAI_API_KEY:\n",
        "        st.warning(\"OpenAI API key not set. Cannot generate advanced insights.\")\n",
        "        return None\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following hotel review. Identify key positive and negative aspects, categorizing them into themes like 'Staff', 'Room', 'Location', 'Food', 'Amenities', 'Cleanliness', 'Value', 'Service', 'Facilities', 'Noise', 'Check-in/Check-out', 'Booking', 'Internet/Wi-Fi', 'Parking', 'Breakfast'.\n",
        "    For each theme, list specific points.\n",
        "    Then, provide actionable recommendations for the hotel based on the review.\n",
        "\n",
        "    Format the output as a JSON object with the following keys:\n",
        "    {{\n",
        "        \"summary\": \"...\",\n",
        "        \"positive_aspects\": [\n",
        "            {{\"theme\": \"...\", \"points\": [\"...\", \"...\"]}},\n",
        "            ...\n",
        "        ],\n",
        "        \"negative_aspects\": [\n",
        "            {{\"theme\": \"...\", \"points\": [\"...\", \"...\"]}},\n",
        "            ...\n",
        "        ],\n",
        "        \"action_items\": [\"...\", \"...\", \"...\"]\n",
        "    }}\n",
        "\n",
        "    Review: \"{review_text}\"\n",
        "    \"\"\"\n",
        "\n",
        "    response_schema = {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"summary\": {\"type\": \"STRING\"},\n",
        "            \"positive_aspects\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"OBJECT\",\n",
        "                    \"properties\": {\n",
        "                        \"theme\": {\"type\": \"STRING\"},\n",
        "                        \"points\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}}\n",
        "                    },\n",
        "                    \"propertyOrdering\": [\"theme\", \"points\"]\n",
        "                }\n",
        "            },\n",
        "            \"negative_aspects\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"OBJECT\",\n",
        "                    \"properties\": {\n",
        "                        \"theme\": {\"type\": \"STRING\"},\n",
        "                        \"points\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}}\n",
        "                    },\n",
        "                    \"propertyOrdering\": [\"theme\", \"points\"]\n",
        "                }\n",
        "            },\n",
        "            \"action_items\": {\n",
        "                \"type\": \"ARRAY\",\n",
        "                \"items\": {\"type\": \"STRING\"}\n",
        "            }\n",
        "        },\n",
        "        \"propertyOrdering\": [\"summary\", \"positive_aspects\", \"negative_aspects\", \"action_items\"]\n",
        "    }\n",
        "\n",
        "    api_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent\"\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'x-goog-api-key': OPENAI_API_KEY\n",
        "    }\n",
        "    payload = {\n",
        "        \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"responseMimeType\": \"application/json\",\n",
        "            \"responseSchema\": response_schema\n",
        "        }\n",
        "    }\n",
        "\n",
        "    retries = 3\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            if result.get('candidates') and result['candidates'][0].get('content') and result['candidates'][0]['content'].get('parts'):\n",
        "                json_string = result['candidates'][0]['content']['parts'][0]['text']\n",
        "                parsed_json = json.loads(json_string)\n",
        "                return parsed_json\n",
        "            else:\n",
        "                st.error(\"Unexpected AI response structure.\")\n",
        "                return None\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            st.warning(f\"Attempt {attempt + 1} failed (Request Error): {req_err}\")\n",
        "        except json.JSONDecodeError as json_err:\n",
        "            st.warning(f\"Attempt {attempt + 1} failed (JSON Decode Error): {json_err}. Response: {response.text}\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Attempt {attempt + 1} failed (General Error): {e}\")\n",
        "\n",
        "        if attempt < retries - 1:\n",
        "            time.sleep(2 ** attempt)\n",
        "        else:\n",
        "            st.error(\"Failed to get insights from AI after multiple retries.\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "# --- 5. Main Streamlit UI ---\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Hotel Review Dashboard\")\n",
        "st.title(\"Hotel Review Insights Dashboard 🏨📊\")\n",
        "st.markdown(\"\"\"\n",
        "_Upload a CSV file containing hotel reviews to get sentiment analysis, AI-generated summaries,\n",
        "thematic insights, and actionable recommendations._\n",
        "\"\"\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload your CSV file (must have a 'Review_Text' column)\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    try:\n",
        "        reviews_df = pd.read_csv(uploaded_file)\n",
        "\n",
        "        if 'Review_Text' not in reviews_df.columns:\n",
        "            st.error(\"CSV must contain a 'Review_Text' column.\")\n",
        "            st.stop()\n",
        "        reviews_df['Review_Text'] = reviews_df['Review_Text'].astype(str).str.strip()\n",
        "        reviews_df = reviews_df[reviews_df['Review_Text'] != ''].reset_index(drop=True)\n",
        "\n",
        "        if reviews_df.empty:\n",
        "            st.warning(\"No valid reviews found in the uploaded file after cleaning.\")\n",
        "            st.stop()\n",
        "\n",
        "        st.success(f\"Successfully loaded {len(reviews_df)} reviews.\")\n",
        "        st.dataframe(reviews_df.head())\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.subheader(\"Processing Reviews...\")\n",
        "\n",
        "        all_sentiments = []\n",
        "        all_positive_probs = []\n",
        "        all_summaries = []\n",
        "        all_positive_aspects = []\n",
        "        all_negative_aspects = []\n",
        "        all_action_items = []\n",
        "\n",
        "        ai_insight_limit = st.slider(\"Number of reviews for AI insights (OpenAI calls can be slow/costly):\",\n",
        "                                     min_value=0, max_value=min(50, len(reviews_df)), value=min(5, len(reviews_df)))\n",
        "\n",
        "        progress_text = \"Analyzing reviews. Please wait...\"\n",
        "        my_bar = st.progress(0, text=progress_text)\n",
        "\n",
        "        for i, row in reviews_df.iterrows():\n",
        "            review_text = row['Review_Text']\n",
        "\n",
        "            sentiment, prob_pos = predict_sentiment_single(review_text)\n",
        "            all_sentiments.append(sentiment)\n",
        "            all_positive_probs.append(prob_pos)\n",
        "\n",
        "            if i < ai_insight_limit:\n",
        "                insights = get_openai_insights_single(review_text)\n",
        "                if insights:\n",
        "                    all_summaries.append(insights.get('summary'))\n",
        "                    all_positive_aspects.append(insights.get('positive_aspects', []))\n",
        "                    all_negative_aspects.append(insights.get('negative_aspects', []))\n",
        "                    all_action_items.append(insights.get('action_items', []))\n",
        "                else:\n",
        "                    all_summaries.append(None)\n",
        "                    all_positive_aspects.append([])\n",
        "                    all_negative_aspects.append([])\n",
        "                    all_action_items.append([])\n",
        "            else:\n",
        "                all_summaries.append(None)\n",
        "                all_positive_aspects.append([])\n",
        "                all_negative_aspects.append([])\n",
        "                all_action_items.append([])\n",
        "\n",
        "            my_bar.progress((i + 1) / len(reviews_df), text=progress_text)\n",
        "\n",
        "        reviews_df['Predicted_Sentiment'] = all_sentiments\n",
        "        reviews_df['Positive_Probability'] = all_positive_probs\n",
        "        reviews_df['AI_Summary'] = all_summaries\n",
        "        reviews_df['AI_Positive_Aspects'] = all_positive_aspects\n",
        "        reviews_df['AI_Negative_Aspects'] = all_negative_aspects\n",
        "        reviews_df['AI_Action_Items'] = all_action_items\n",
        "\n",
        "        st.success(\"Review processing complete!\")\n",
        "\n",
        "        # --- Conditional Display of Dashboard Elements ---\n",
        "        if len(reviews_df) < MIN_REVIEWS_FOR_DASHBOARD:\n",
        "            st.warning(f\"Upload at least {MIN_REVIEWS_FOR_DASHBOARD} reviews to see the full dashboard insights.\")\n",
        "            st.dataframe(reviews_df[['Review_Text', 'Predicted_Sentiment', 'Positive_Probability']].head())\n",
        "        else:\n",
        "            # --- Overall Property Score (0-10) ---\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Overall Property Sentiment Score\")\n",
        "            numeric_sentiments = reviews_df['Predicted_Sentiment'].map({'Positive': 1, 'Negative': 0})\n",
        "            overall_avg_sentiment = numeric_sentiments.mean() if not numeric_sentiments.empty else 0\n",
        "            property_score = round(overall_avg_sentiment * 10, 1) # Scale to 0-10\n",
        "\n",
        "            st.metric(label=\"Property Sentiment Score (0-10)\", value=f\"{property_score}/10\")\n",
        "            if property_score >= 7.5:\n",
        "                st.success(\"Great job! Keep up the excellent work. 👍\")\n",
        "            elif property_score >= 5.0:\n",
        "                st.info(\"Good performance, but there's room for improvement. 📈\")\n",
        "            else:\n",
        "                st.error(\"Attention needed! Significant improvements are recommended. 🚨\")\n",
        "\n",
        "            # --- Dashboard Layout ---\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Dashboard Overview\")\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "\n",
        "            with col1:\n",
        "                st.markdown(\"#### Predicted Sentiment Distribution\")\n",
        "                sentiment_counts = reviews_df['Predicted_Sentiment'].value_counts()\n",
        "                fig_dist, ax_dist = plt.subplots(figsize=(5, 3))\n",
        "                sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, ax=ax_dist, palette={'Positive': 'green', 'Negative': 'red'})\n",
        "                ax_dist.set_title('Sentiment Counts')\n",
        "                ax_dist.set_xlabel('Sentiment')\n",
        "                ax_dist.set_ylabel('Number of Reviews')\n",
        "                st.pyplot(fig_dist)\n",
        "\n",
        "                st.markdown(\"#### Word Clouds\")\n",
        "                positive_reviews_text = \" \".join(reviews_df[reviews_df['Predicted_Sentiment'] == 'Positive']['Review_Text'].tolist())\n",
        "                negative_reviews_text = \" \".join(reviews_df[reviews_df['Predicted_Sentiment'] == 'Negative']['Review_Text'].tolist())\n",
        "\n",
        "                fig_wc, axes_wc = plt.subplots(2, 1, figsize=(5, 5))\n",
        "\n",
        "                # Positive Word Cloud\n",
        "                if positive_reviews_text.strip():\n",
        "                    wordcloud_positive = WordCloud(width=400, height=200, background_color='white', stopwords=stop_words).generate(positive_reviews_text)\n",
        "                    axes_wc[0].imshow(wordcloud_positive, interpolation='bilinear')\n",
        "                    axes_wc[0].set_title('Positive Reviews')\n",
        "                    axes_wc[0].axis('off')\n",
        "                else:\n",
        "                    axes_wc[0].text(0.5, 0.5, \"No Positive Reviews\", ha='center', va='center', fontsize=12, color='gray')\n",
        "                    axes_wc[0].axis('off')\n",
        "\n",
        "                # Negative Word Cloud\n",
        "                if negative_reviews_text.strip():\n",
        "                    wordcloud_negative = WordCloud(width=400, height=200, background_color='black', stopwords=stop_words, colormap='Reds').generate(negative_reviews_text)\n",
        "                    axes_wc[1].imshow(wordcloud_negative, interpolation='bilinear')\n",
        "                    axes_wc[1].set_title('Negative Reviews')\n",
        "                    axes_wc[1].axis('off')\n",
        "                else:\n",
        "                    axes_wc[1].text(0.5, 0.5, \"No Negative Reviews\", ha='center', va='center', fontsize=12, color='gray')\n",
        "                    axes_wc[1].axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                st.pyplot(fig_wc)\n",
        "\n",
        "            with col2:\n",
        "                st.markdown(\"#### Aggregated Thematic Insights\")\n",
        "                all_pos_themes = {}\n",
        "                for aspects_list in reviews_df['AI_Positive_Aspects'].dropna():\n",
        "                    for aspect in aspects_list:\n",
        "                        theme = aspect.get('theme', 'Other Positive')\n",
        "                        all_pos_themes[theme] = all_pos_themes.get(theme, 0) + len(aspect.get('points', []))\n",
        "\n",
        "                all_neg_themes = {}\n",
        "                for aspects_list in reviews_df['AI_Negative_Aspects'].dropna():\n",
        "                    for aspect in aspects_list:\n",
        "                        theme = aspect.get('theme', 'Other Negative')\n",
        "                        all_neg_themes[theme] = all_neg_themes.get(theme, 0) + len(aspect.get('points', []))\n",
        "\n",
        "                if all_pos_themes or all_neg_themes:\n",
        "                    theme_data = []\n",
        "                    all_themes = sorted(list(set(list(all_pos_themes.keys()) + list(all_neg_themes.keys()))))\n",
        "                    for theme in all_themes:\n",
        "                        theme_data.append({\n",
        "                            'Theme': theme,\n",
        "                            'Positive Mentions': all_pos_themes.get(theme, 0),\n",
        "                            'Negative Mentions': all_neg_themes.get(theme, 0)\n",
        "                        })\n",
        "                    theme_df = pd.DataFrame(theme_data).set_index('Theme')\n",
        "                    st.dataframe(theme_df)\n",
        "\n",
        "                    fig_themes, ax_themes = plt.subplots(figsize=(8, 4))\n",
        "                    theme_df.plot(kind='bar', ax=ax_themes, color=['green', 'red'])\n",
        "                    ax_themes.set_title('Mentions by Theme')\n",
        "                    ax_themes.set_ylabel('Number of Mentions')\n",
        "                    ax_themes.tick_params(axis='x', rotation=45)\n",
        "                    plt.tight_layout()\n",
        "                    st.pyplot(fig_themes)\n",
        "\n",
        "                else:\n",
        "                    st.info(\"No AI insights generated for thematic analysis (adjust slider or upload more reviews).\")\n",
        "\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Individual Review Insights\")\n",
        "            for i, row in reviews_df.iterrows():\n",
        "                with st.expander(f\"Review {i+1} - Predicted: {row['Predicted_Sentiment']} (Prob: {row['Positive_Probability']:.2f})\"):\n",
        "                    st.write(f\"**Review Text:** {row['Review_Text']}\")\n",
        "                    st.write(f\"**Predicted Sentiment:** {row['Predicted_Sentiment']}\")\n",
        "                    st.write(f\"**Positive Probability:** {row['Positive_Probability']:.2f}\")\n",
        "\n",
        "                    if row['AI_Summary']:\n",
        "                        st.write(f\"**AI Summary:** {row['AI_Summary']}\")\n",
        "\n",
        "                        st.markdown(\"##### AI Positive Aspects:\")\n",
        "                        if row['AI_Positive_Aspects']:\n",
        "                            for aspect in row['AI_Positive_Aspects']:\n",
        "                                st.markdown(f\"**{aspect.get('theme', 'N/A')}:**\")\n",
        "                                for point in aspect.get('points', []):\n",
        "                                    st.write(f\"- {point}\")\n",
        "                        else:\n",
        "                            st.write(\"No specific positive aspects identified by AI.\")\n",
        "\n",
        "                        st.markdown(\"##### AI Negative Aspects:\")\n",
        "                        if row['AI_Negative_Aspects']:\n",
        "                            for aspect in row['AI_Negative_Aspects']:\n",
        "                                st.markdown(f\"**{aspect.get('theme', 'N/A')}:**\")\n",
        "                                for point in aspect.get('points', []):\n",
        "                                    st.write(f\"- {point}\")\n",
        "                        else:\n",
        "                            st.write(\"No specific negative aspects identified by AI.\")\n",
        "\n",
        "                        st.markdown(\"##### AI Actionable Recommendations:\")\n",
        "                        if row['AI_Action_Items']:\n",
        "                            for item in row['AI_Action_Items']:\n",
        "                                st.write(f\"- {item}\")\n",
        "                        else:\n",
        "                            st.write(\"No specific action items identified by AI.\")\n",
        "                    else:\n",
        "                        st.info(\"AI insights not generated for this review (limit reached or API error).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred: {e}\")\n",
        "        st.info(\"Please check your CSV file format and try again.\")"
      ],
      "metadata": {
        "id": "s3PgnVGRwCNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "from google.colab import userdata # Import userdata here\n",
        "\n",
        "# Terminate any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# --- Retrieve API Key from Colab Secrets and set as Environment Variable ---\n",
        "try:\n",
        "    colab_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if colab_api_key:\n",
        "        os.environ['OPENAI_API_KEY'] = colab_api_key # Set as environment variable\n",
        "        print(\"API key successfully loaded from Colab Secrets and set as environment variable.\")\n",
        "    else:\n",
        "        print(\"Warning: 'OPENAI_API_KEY' not found in Colab Secrets. AI insights will not work.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving API key from Colab Secrets: {e}. AI insights will not work.\")\n",
        "    os.environ['OPENAI_API_KEY'] = \"\" # Ensure it's empty if retrieval fails\n",
        "\n",
        "# Start Streamlit in the background\n",
        "print(\"Starting Streamlit app in the background...\")\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(5) # Give Streamlit a moment to start\n",
        "\n",
        "# Set your ngrok authtoken (REQUIRED!)\n",
        "# Get it from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# ngrok.set_auth_token(\"YOUR_NGROK_AUTHTOKEN\") # Uncomment and paste your authtoken here\n",
        "\n",
        "# Open a ngrok tunnel to the Streamlit port\n",
        "print(\"Opening ngrok tunnel...\")\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Your Streamlit app is live at: {public_url}\")\n",
        "\n",
        "# To keep the Colab cell running and the tunnel active, you can monitor the process\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Streamlit app stopped.\")\n",
        "    process.terminate()\n",
        "    ngrok.kill()"
      ],
      "metadata": {
        "id": "yiRUEkmOwFSN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}